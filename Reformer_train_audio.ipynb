{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reformer_returns",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyObhfXjxHuRrMsfXygN/YZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniketRajpoot/reformer-pytorch/blob/master/Reformer_train_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpexnLdQlyUl",
        "outputId": "674acb1f-5eed-4e5b-f856-f630c7c591aa"
      },
      "source": [
        "!git clone https://github.com/AniketRajpoot/reformer-pytorch.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'reformer-pytorch'...\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 1028 (delta 107), reused 73 (delta 39), pack-reused 857\u001b[K\n",
            "Receiving objects: 100% (1028/1028), 35.36 MiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (647/647), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bExUk9ixprN7",
        "outputId": "52e6afc5-5b7c-46a9-ab79-03820a46282b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/reformer-pytorch/examples/enwik8_deepspeed/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/reformer-pytorch/examples/enwik8_deepspeed/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCsyQYtbqJ5X",
        "outputId": "7089265b-e198-48cb-80cc-36ffdff108b7"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mreformer-pytorch\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKL7loZnl9PY",
        "outputId": "b012eb2a-1a21-445d-ad81-7e43abcc30f9"
      },
      "source": [
        "%cd /content/reformer-pytorch/examples/enwik8_deepspeed"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/reformer-pytorch/examples/enwik8_deepspeed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Naaqi35l-Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4147803-2941-4bc4-b133-cedd7b6bbf40"
      },
      "source": [
        "!pip install deepspeed\r\n",
        "!pip install better_exceptions\r\n",
        "!pip install av\r\n",
        "!pip install soundfile\r\n",
        "!pip install mpi4py\r\n",
        "!pip install reformer_pytorch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeed\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/e7/1a7af3bff34380dc3806ae4e3d803d69ab842e71fb1c323a68f6c339b903/deepspeed-0.3.9.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (0.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deepspeed) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 26.6MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.4.0->deepspeed) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (51.1.1)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.9-cp36-none-any.whl size=265998 sha256=e5df711c1e4f5a0e6b3037938ea3ae5b1638f40ead9128820cab16831dd56857\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/f9/46/9fbf1165ff87b49f88f0357f36d68132c63f94eaf93a3f034e\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.9 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting better_exceptions\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/6f/f565e62cf4c4200c50b8eb52089339d02aaa60799be9ae75a6756bbc8d6e/better_exceptions-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: better-exceptions\n",
            "Successfully installed better-exceptions-0.3.2\n",
            "Collecting av\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
            "\u001b[K     |████████████████████████████████| 36.9MB 1.2MB/s \n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-8.0.2\n",
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n",
            "Collecting mpi4py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 13.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2074453 sha256=702a41514cdef2b5b06796c824f3964f7f565c90a16fb79d5d825f428bd485fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e0/86/2b713dd512199096012ceca61429e12b960888de59818871d6\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.0.3\n",
            "Collecting reformer_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/16/e84a99e6d34b616ab95ed6ab8c1b76f0db50e3beea854879384602e50e54/reformer_pytorch-1.2.4-py3-none-any.whl\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from reformer_pytorch) (1.7.0+cu101)\n",
            "Collecting product-key-memory\n",
            "  Downloading https://files.pythonhosted.org/packages/31/3b/c1f8977e4b04f047acc7b23c7424d1e2e624ed7031e699a2ac2287af4c1f/product_key_memory-0.1.10.tar.gz\n",
            "Collecting local-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/37/f8702c01f3f2af43a967d6a45bca88529f8fdaa6fc2175377bf8ca2000ee/local_attention-1.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->reformer_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->reformer_pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->reformer_pytorch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->reformer_pytorch) (1.19.4)\n",
            "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp36-none-any.whl size=2904 sha256=69f48872a741b865007301f4e9437abaaff3ed897dcb1b578f2ee8c156bb61fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "  Building wheel for product-key-memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-cp36-none-any.whl size=3072 sha256=3e0db25a895af20caa6e75d9624658211bb21db60aecdc8f9b8482930495ff0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/e0/3b/fd3111a4fac652ed014ccfd4757754f006132723985e229419\n",
            "Successfully built axial-positional-embedding product-key-memory\n",
            "Installing collected packages: axial-positional-embedding, product-key-memory, local-attention, reformer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 local-attention-1.2.1 product-key-memory-0.1.10 reformer-pytorch-1.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgw5oj2El-Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbb3b16-9ba6-43b4-b408-c10e6a68eb22"
      },
      "source": [
        "!deepspeed train.py --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-01-08 19:09:16,050] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-01-08 19:09:16,077] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train.py --deepspeed --deepspeed_config ds_config.json\n",
            "[2021-01-08 19:09:16,998] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.7.8\n",
            "[2021-01-08 19:09:16,998] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-01-08 19:09:16,998] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-01-08 19:09:16,998] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-01-08 19:09:16,999] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2021-01-08 19:09:16,999] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2021-01-08 19:09:17.328221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Found 147 files!\n",
            "Length of dataset is  52853\n",
            "Loading vqvae in train mode\n",
            "{'l2': 0.005508611572464147, 'l1': 0.04543668434957673, 'spec': 3007.8655752866075}\n",
            "retrieved\n",
            "[2021-01-08 19:11:20,609] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.9, git-hash=unknown, git-branch=unknown\n",
            "[2021-01-08 19:11:20,609] [INFO] [distributed.py:38:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2021-01-08 19:11:20,658] [WARNING] [config.py:48:read_zero_config_deprecated] DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: \n",
            "ZeRO optimization should be enabled as:\n",
            "\"session_params\": {\n",
            "  \"zero_optimization\": {\n",
            "    \"stage\": [0|1|2],\n",
            "    \"allgather_partitions\": [true|false],\n",
            "    \"allgather_bucket_size\": 500000000,\n",
            "    \"reduce_scatter\": [true|false],\n",
            "    \"contiguous_gradients\" : [true|false]\n",
            "    \"overlap_comm\": [true|false],\n",
            "    \"reduce_bucket_size\": 500000000\n",
            "    \"load_from_fp32_weights\": [true|false]\n",
            "    \"cpu_offload\": [true|false]\n",
            "    }\n",
            "}\n",
            "\n",
            "[2021-01-08 19:11:20,664] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -I/usr/local/lib/python3.6/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.6/dist-packages/torch/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.6/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -std=c++14 -c /usr/local/lib/python3.6/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -I/usr/local/lib/python3.6/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.6/dist-packages/torch/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.6/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.6/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 18.240922927856445 seconds\n",
            "[2021-01-08 19:11:40,215] [INFO] [engine.py:519:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-01-08 19:11:40,215] [INFO] [engine.py:522:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.8, 0.999]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.0002\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
            "[2021-01-08 19:11:40,215] [INFO] [engine.py:640:_configure_zero_optimizer] Creating fp16 ZeRO stage 1 optimizer\n",
            "[2021-01-08 19:11:40,216] [INFO] [stage1.py:152:__init__] ZeRO Elastic Checkpoint = True\n",
            "[2021-01-08 19:11:40,216] [INFO] [logging.py:60:log_dist] [Rank 0] Using default max_elements_per_comm 500000000\n",
            "[2021-01-08 19:11:40,216] [INFO] [logging.py:60:log_dist] [Rank 0] Total number of elements in model: 13773824, max elements per com: 500000000\n",
            "[2021-01-08 19:11:40,216] [INFO] [logging.py:60:log_dist] [Rank 0] sub_partition_count: 1, sub_partition_size: 13773824, padding: 0\n",
            "[2021-01-08 19:11:40,216] [INFO] [logging.py:60:log_dist] [Rank 0] number of elements with padding: 13773824 + 0 = 13773824\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:367:get_data_parallel_sub_partitions] **** partition info:\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:368:get_data_parallel_sub_partitions] \t total_num_elements=13773824\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:369:get_data_parallel_sub_partitions] \t world_size=1\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:370:get_data_parallel_sub_partitions] \t max_elements_per_comm=13773824\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:371:get_data_parallel_sub_partitions] \t sub_partition_size=13773824\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:372:get_data_parallel_sub_partitions] \t num_sub_partitions=1\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:373:get_data_parallel_sub_partitions] \t num_comm_intervals=1\n",
            "[2021-01-08 19:11:40,230] [INFO] [stage1.py:374:get_data_parallel_sub_partitions] ****\n",
            "[2021-01-08 19:11:42,337] [INFO] [engine.py:552:_configure_optimizer] DeepSpeed Final Optimizer = <deepspeed.runtime.zero.stage1.FP16_DeepSpeedZeroOptimizer_Stage1 object at 0x7fdf0d4e9198>\n",
            "[2021-01-08 19:11:42,383] [INFO] [engine.py:553:_configure_optimizer] DeepSpeed Final Optimizer = {'loss_scaler': <deepspeed.runtime.fp16.loss_scaler.DynamicLossScaler object at 0x7fdf0d4e97b8>, 'dynamic_loss_scale': True, 'overflow': False, 'base_optimizer_state': [[{'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}]], 'zero_stage': 1, 'partition_count': 1, 'num_comm_intervals_per_group': [1], 'local_sub_partitions_of_fp32_groups': [[tensor([ 0.5176, -0.3064, -2.3027,  ...,  0.0275,  0.0025,  0.0231],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)]]}\n",
            "[2021-01-08 19:11:42,383] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-01-08 19:11:42,383] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fdf772125c0>\n",
            "[2021-01-08 19:11:42,383] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[[0.8, 0.999]]\n",
            "[2021-01-08 19:11:42,383] [INFO] [config.py:705:print] DeepSpeedEngine configuration:\n",
            "[2021-01-08 19:11:42,383] [INFO] [config.py:709:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7fdf77212128>\n",
            "[2021-01-08 19:11:42,383] [INFO] [config.py:709:print]   allreduce_always_fp32 ........ False\n",
            "[2021-01-08 19:11:42,383] [INFO] [config.py:709:print]   amp_enabled .................. False\n",
            "[2021-01-08 19:11:42,383] [INFO] [config.py:709:print]   amp_params ................... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   disable_allgather ............ False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   dump_state ................... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   elasticity_enabled ........... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   fp16_enabled ................. True\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   global_rank .................. 0\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   gradient_accumulation_steps .. 4\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   gradient_clipping ............ 0.0\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   loss_scale ................... 0\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   memory_breakdown ............. False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   optimizer_name ............... adam\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   optimizer_params ............. {'lr': 0.0002, 'betas': [0.8, 0.999], 'eps': 1e-08, 'adam_w_mode': True}\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   pld_enabled .................. False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   pld_params ................... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   prescale_gradients ........... False\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   scheduler_name ............... WarmupLR\n",
            "[2021-01-08 19:11:42,384] [INFO] [config.py:709:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0002, 'warmup_num_steps': 1000}\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   sparse_attention ............. None\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   sparse_gradients_enabled ..... False\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   steps_per_print .............. 2000\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   tensorboard_enabled .......... False\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   tensorboard_output_path ...... \n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   train_batch_size ............. 32\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   train_micro_batch_size_per_gpu  8\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   wall_clock_breakdown ......... False\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   world_size ................... 1\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   zero_allow_untested_optimizer  False\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 500000000,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": false,\n",
            "    \"cpu_offload\": false,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"overlap_comm\": false,\n",
            "    \"reduce_bucket_size\": 500000000,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 1\n",
            "}\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   zero_enabled ................. True\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:709:print]   zero_optimization_stage ...... 1\n",
            "[2021-01-08 19:11:42,385] [INFO] [config.py:716:print]   json = {\n",
            "    \"fp16\":{\n",
            "        \"enabled\":true,\n",
            "        \"hysteresis\":2,\n",
            "        \"loss_scale\":0,\n",
            "        \"loss_scale_window\":1000,\n",
            "        \"min_loss_scale\":1\n",
            "    },\n",
            "    \"gradient_accumulation_steps\":4,\n",
            "    \"max_grad_norm\":0.5,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"adam_w_mode\":true,\n",
            "            \"betas\":[\n",
            "                0.8,\n",
            "                0.999\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":0.0002\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":0.0002,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":1000\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":2000,\n",
            "    \"train_batch_size\":32,\n",
            "    \"wall_clock_breakdown\":false,\n",
            "    \"zero_optimization\":true\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.6/dist-packages/torch/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.6/dist-packages/torch/include/THC -isystem /usr/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.6/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 13.044800758361816 seconds\n",
            "  0% 0/5000 [00:00<?, ?it/s]26.8125\n",
            "26.8125\n",
            "26.8125\n",
            "[2021-01-08 19:11:56,364] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296\n",
            "26.828125\n",
            "26.828125\n",
            "26.828125\n",
            "26.8125\n",
            "[2021-01-08 19:11:56,856] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
            "26.8125\n",
            "26.8125\n",
            "26.828125\n",
            "26.828125\n",
            "[2021-01-08 19:11:57,355] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
            "26.828125\n",
            "26.84375\n",
            "26.84375\n",
            "26.828125\n",
            "[2021-01-08 19:11:57,849] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
            "26.828125\n",
            "26.828125\n",
            "26.84375\n",
            "26.84375\n",
            "[2021-01-08 19:11:58,345] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n",
            "26.84375\n",
            "26.828125\n",
            "26.828125\n",
            "26.828125\n",
            "[2021-01-08 19:11:58,836] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n",
            "26.84375\n",
            "26.84375\n",
            "26.84375\n",
            "26.84375\n",
            "[2021-01-08 19:11:59,325] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n",
            "26.828125\n",
            "26.859375\n",
            "26.84375\n",
            "26.84375\n",
            "[2021-01-08 19:11:59,811] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n",
            "26.84375\n",
            "26.84375\n",
            "26.859375\n",
            "26.859375\n",
            "[2021-01-08 19:12:00,307] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n",
            "26.84375\n",
            "26.84375\n",
            "26.84375\n",
            "26.84375\n",
            "[2021-01-08 19:12:00,793] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n",
            "26.859375\n",
            "26.859375\n",
            "26.84375\n",
            "26.84375\n",
            "[2021-01-08 19:12:01,280] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n",
            "26.84375\n",
            "26.859375\n",
            "26.859375\n",
            "26.859375\n",
            "[2021-01-08 19:12:01,782] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n",
            "26.84375\n",
            "26.84375\n",
            "26.84375\n",
            "26.859375\n",
            "[2021-01-08 19:12:02,283] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n",
            "26.859375\n",
            "26.859375\n",
            "26.859375\n",
            "26.84375\n",
            "[2021-01-08 19:12:02,778] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
            "26.84375\n",
            "26.859375\n",
            "26.859375\n",
            "26.859375\n",
            "[2021-01-08 19:12:03,268] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n",
            "26.859375\n",
            "26.875\n",
            "26.875\n",
            "26.859375\n",
            "[2021-01-08 19:12:03,758] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n",
            "26.859375\n",
            "26.859375\n",
            "26.859375\n",
            "26.875\n",
            "[2021-01-08 19:12:04,249] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0\n",
            "26.875\n",
            "26.859375\n",
            "26.859375\n",
            "26.890625\n",
            "[2021-01-08 19:12:04,752] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n",
            "26.875\n",
            "26.875\n",
            "26.875\n",
            "26.875\n",
            "[2021-01-08 19:12:05,243] [INFO] [stage1.py:636:step] [deepspeed] OVERFLOW! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "26.890625\n",
            "26.890625\n",
            "26.875\n",
            "26.875\n",
            "26.875\n",
            "5.72265625\n",
            "5.72265625\n",
            "5.7265625\n",
            "5.71875\n",
            "5.72265625\n",
            "5.72265625\n",
            "5.71484375\n",
            "5.71875\n",
            "4.01171875\n",
            "4.0078125\n",
            "4.0078125\n",
            "4.00390625\n",
            "2.11328125\n",
            "2.11328125\n",
            "2.1171875\n",
            "2.115234375\n",
            "0.86669921875\n",
            "0.86572265625\n",
            "0.8662109375\n",
            "0.8671875\n",
            "training loss: 0.0 :: epoch : 0\n",
            "=> Saving checkpoint\n",
            "  0% 1/5000 [00:14<19:53:21, 14.32s/it]0.327392578125\n",
            "0.328125\n",
            "0.328125\n",
            "0.3271484375\n",
            "0.134521484375\n",
            "0.134521484375\n",
            "0.134765625\n",
            "0.134521484375\n",
            "0.062042236328125\n",
            "0.061859130859375\n",
            "0.06195068359375\n",
            "0.06201171875\n",
            "0.03277587890625\n",
            "0.032745361328125\n",
            "0.03277587890625\n",
            "0.032745361328125\n",
            "0.019500732421875\n",
            "0.0195159912109375\n",
            "0.0194854736328125\n",
            "0.01953125\n",
            "0.01282501220703125\n",
            "0.012847900390625\n",
            "0.012847900390625\n",
            "0.0128326416015625\n",
            "0.00914764404296875\n",
            "0.0091552734375\n",
            "0.0091400146484375\n",
            "0.0091400146484375\n",
            "0.006938934326171875\n",
            "0.006931304931640625\n",
            "0.006923675537109375\n",
            "0.006923675537109375\n",
            "0.005519866943359375\n",
            "0.00551605224609375\n",
            "0.00551605224609375\n",
            "0.0055084228515625\n",
            "0.004589080810546875\n",
            "0.00458526611328125\n",
            "0.00457763671875\n",
            "0.00458526611328125\n",
            "0.003948211669921875\n",
            "0.003948211669921875\n",
            "0.003940582275390625\n",
            "0.00394439697265625\n",
            "0.003498077392578125\n",
            "0.003490447998046875\n",
            "0.003498077392578125\n",
            "0.003498077392578125\n",
            "0.00316619873046875\n",
            "0.0031566619873046875\n",
            "0.0031585693359375\n",
            "0.0031642913818359375\n",
            "0.0029163360595703125\n",
            "0.002910614013671875\n",
            "0.0029125213623046875\n",
            "0.0029125213623046875\n",
            "0.002719879150390625\n",
            "0.002716064453125\n",
            "0.0027179718017578125\n",
            "0.0027141571044921875\n",
            "0.0025691986083984375\n",
            "0.0025691986083984375\n",
            "0.002567291259765625\n",
            "0.002567291259765625\n",
            "0.00244903564453125\n",
            "0.0024471282958984375\n",
            "0.002445220947265625\n",
            "0.0024471282958984375\n",
            "0.002353668212890625\n",
            "0.0023517608642578125\n",
            "0.002349853515625\n",
            "0.0023517608642578125\n",
            "0.002277374267578125\n",
            "0.002277374267578125\n",
            "0.0022735595703125\n",
            "0.0022754669189453125\n",
            "0.0022106170654296875\n",
            "0.0022125244140625\n",
            "0.0022106170654296875\n",
            "0.0022125244140625\n",
            "0.002162933349609375\n",
            "0.00215911865234375\n",
            "0.00215911865234375\n",
            "0.00215911865234375\n",
            "0.0021190643310546875\n",
            "0.0021209716796875\n",
            "0.002117156982421875\n",
            "0.0021190643310546875\n",
            "0.00208282470703125\n",
            "0.002079010009765625\n",
            "0.0020809173583984375\n",
            "0.00208282470703125\n",
            "0.00205230712890625\n",
            "0.0020542144775390625\n",
            "0.0020503997802734375\n",
            "0.0020503997802734375\n",
            "0.002025604248046875\n",
            "0.002025604248046875\n",
            "0.002025604248046875\n",
            "0.002025604248046875\n",
            "  0% 2/5000 [00:27<19:35:08, 14.11s/it]0.002025604248046875\n",
            "0.002025604248046875\n",
            "0.002025604248046875\n",
            "0.0020236968994140625\n",
            "0.00200653076171875\n",
            "0.0020046234130859375\n",
            "0.002002716064453125\n",
            "0.002002716064453125\n",
            "0.0019893646240234375\n",
            "0.001987457275390625\n",
            "0.0019893646240234375\n",
            "0.001987457275390625\n",
            "0.0019741058349609375\n",
            "0.0019741058349609375\n",
            "0.0019741058349609375\n",
            "0.001972198486328125\n",
            "0.0019626617431640625\n",
            "0.001956939697265625\n",
            "0.001956939697265625\n",
            "0.001956939697265625\n",
            "0.0019445419311523438\n",
            "0.0019426345825195312\n",
            "0.0019445419311523438\n",
            "0.0019426345825195312\n",
            "0.0019330978393554688\n",
            "0.001934051513671875\n",
            "0.001934051513671875\n",
            "0.0019330978393554688\n",
            "0.0019216537475585938\n",
            "0.001922607421875\n",
            "0.0019254684448242188\n",
            "0.001922607421875\n",
            "0.001911163330078125\n",
            "0.0019092559814453125\n",
            "0.0019121170043945312\n",
            "0.0019092559814453125\n",
            "0.0019006729125976562\n",
            "0.0019006729125976562\n",
            "0.0019016265869140625\n",
            "0.0019006729125976562\n",
            "0.00189208984375\n",
            "0.0018930435180664062\n",
            "0.0018901824951171875\n",
            "0.0018901824951171875\n",
            "0.0018796920776367188\n",
            "0.0018815994262695312\n",
            "0.0018787384033203125\n",
            "0.0018777847290039062\n",
            "0.0018711090087890625\n",
            "0.0018682479858398438\n",
            "0.0018711090087890625\n",
            "0.00186920166015625\n",
            "0.0018634796142578125\n",
            "0.0018606185913085938\n",
            "0.0018634796142578125\n",
            "0.001861572265625\n",
            "0.0018529891967773438\n",
            "0.0018510818481445312\n",
            "0.0018520355224609375\n",
            "0.0018510818481445312\n",
            "0.0018434524536132812\n",
            "0.0018415451049804688\n",
            "0.0018444061279296875\n",
            "0.0018434524536132812\n",
            "0.0018358230590820312\n",
            "0.0018329620361328125\n",
            "0.0018339157104492188\n",
            "0.0018339157104492188\n",
            "0.0018262863159179688\n",
            "0.0018243789672851562\n",
            "0.0018243789672851562\n",
            "0.00182342529296875\n",
            "0.0018186569213867188\n",
            "0.0018157958984375\n",
            "0.0018148422241210938\n",
            "0.0018167495727539062\n",
            "0.0018091201782226562\n",
            "0.0018091201782226562\n",
            "0.001811981201171875\n",
            "0.0018072128295898438\n",
            "0.001796722412109375\n",
            "0.0017976760864257812\n",
            "0.0017976760864257812\n",
            "0.0017976760864257812\n",
            "0.0017919540405273438\n",
            "0.0017900466918945312\n",
            "0.0017900466918945312\n",
            "0.0017881393432617188\n",
            "0.0017805099487304688\n",
            "0.001781463623046875\n",
            "0.001781463623046875\n",
            "0.0017795562744140625\n",
            "0.0017719268798828125\n",
            "0.001773834228515625\n",
            "0.0017719268798828125\n",
            "0.0017719268798828125\n",
            "0.0017642974853515625\n",
            "0.0017642974853515625\n",
            "0.0017633438110351562\n",
            "0.0017633438110351562\n",
            "  0% 3/5000 [00:40<19:00:12, 13.69s/it]0.00177764892578125\n",
            "0.0017766952514648438\n",
            "0.0017786026000976562\n",
            "0.0017757415771484375\n",
            "0.0017671585083007812\n",
            "0.0017681121826171875\n",
            "0.0017652511596679688\n",
            "0.001766204833984375\n",
            "0.0017576217651367188\n",
            "0.0017576217651367188\n",
            "0.0017576217651367188\n",
            "0.0017576217651367188\n",
            "0.0017499923706054688\n",
            "0.0017499923706054688\n",
            "0.001750946044921875\n",
            "0.0017490386962890625\n",
            "0.0017423629760742188\n",
            "0.0017404556274414062\n",
            "0.0017414093017578125\n",
            "0.0017404556274414062\n",
            "0.0017328262329101562\n",
            "0.0017299652099609375\n",
            "0.0017328262329101562\n",
            "0.0017299652099609375\n",
            "0.001720428466796875\n",
            "0.0017232894897460938\n",
            "0.0017242431640625\n",
            "0.0017213821411132812\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deepspeed/launcher/launch.py\", line 133, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deepspeed/launcher/launch.py\", line 129, in main\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1477, in wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 2033, in <module>\n",
            "    model_engine.backward(loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deepspeed/runtime/engine.py\", line 847, in backward\n",
            "    self.optimizer.backward(loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deepspeed/runtime/zero/stage1.py\", line 732, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 53, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 221, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 132, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "  0% 3/5000 [00:44<20:35:35, 14.84s/it]\n",
            "[6e35a9123acf:00653] *** Process received signal ***\n",
            "[6e35a9123acf:00653] Signal: Segmentation fault (11)\n",
            "[6e35a9123acf:00653] Signal code: Address not mapped (1)\n",
            "[6e35a9123acf:00653] Failing at address: 0x7f937aef920d\n",
            "[6e35a9123acf:00653] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f937dba0980]\n",
            "[6e35a9123acf:00653] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f937d7df8a5]\n",
            "[6e35a9123acf:00653] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f937e04ae44]\n",
            "[6e35a9123acf:00653] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f937d7e0735]\n",
            "[6e35a9123acf:00653] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f937e048cb3]\n",
            "[6e35a9123acf:00653] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTC9vqpUl-ay"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjfDrKeQl-c3"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5MPQ0Il-e4"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9ehyPIFl-g7"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7DY2hOol-i2"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ6C1yOQl-k4"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3-GHJh6l-mz"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ8Peh6ul-o_"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}